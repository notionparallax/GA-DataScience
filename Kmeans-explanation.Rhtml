<html>

<head>
<title>K Means example | R</title>
</head>

<body>
<p>K Means is a clustering method to <em>find</em> groups in data.</p>
<p>Begin by importing the <code>stats</code> library, which contains the <code>kmeans</code> implementation we'll be using.</p>
<!--begin.rcode import some libraries
if("stats" %in% rownames(installed.packages()) == FALSE) {install.packages("stats")} #this line checks if the package is installed and if it isn't, installs it.
library(stats)
if("ggplot2" %in% rownames(installed.packages()) == FALSE) {install.packages("ggplot2")}
library(ggplot2)
set.seed(1)
end.rcode-->

<p>For our first example, let's create some synthetic easy-to-cluster data. This generates 4 little clouds of points , each as a dataframe, and joins them all into one data frame. (Adding more rows onto the bottom, not more collumns.)</p>
<!--begin.rcode make some example data
d <- data.frame()
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(1, each=20))))
d <- rbind(d, data.frame(x=1 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(2, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=1 + rnorm(20, 0, 0.1), label=as.factor(rep(3, each=20))))
d <- rbind(d, data.frame(x=3 + rnorm(20, 0, 0.1), y=3 + rnorm(20, 0, 0.1), label=as.factor(rep(4, each=20))))
end.rcode-->
<p>This ends up with a dataframe that looks something like this:</p>
<!--begin.rcode
head(d)
end.rcode-->
<p>have a look...this looks easy enough</p>
<!--begin.rcode show the base data
#TODO: this plot isn't showing any more, fix it
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=label)) + ggtitle('d -- easy clusters')
end.rcode-->
<p>perform clustering</p>
<!--begin.rcode
result1 <- kmeans(d[,1:2], 4)
end.rcode-->
<p><code>kmeans(x, centers, ...</code></p>
<table>
<tr><td>x</td><td>numeric matrix of data, or an object that can be coerced to such a matrix (such as a numeric vector or a data frame with all numeric columns).</td></tr>
<tr><td>centers</td><td>either the number of clusters, say k, or a set of initial (distinct) cluster centres. If a number, a random set of (distinct) rows in x is chosen as the initial centres.</td></tr>
<table>

<p>lets compare a few cluster solutions</p>
<!--begin.rcode
result12 <- kmeans(d[,1:2], 2)
d$cluster12 <- as.factor(result12$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster12)) + ggtitle('kmeans result1 -- (k=2)')
end.rcode-->
<p>With two centroids searching for clusters to own it tends to pick two of the obvious clusters and treat them as one.</p>
<!--begin.rcode
result13 <- kmeans(d[,1:2], 3)
d$cluster13 <- as.factor(result13$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster13)) + ggtitle('kmeans result1 -- (k=3)')
end.rcode-->
<p>With three centres it finds two of the obvious clusters, but groups the remaining two together</p>
<!--begin.rcode
result14 <- kmeans(d[,1:2], 4)
d$cluster14 <- as.factor(result14$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster14)) + ggtitle('kmeans result1 -- (k=4)')
end.rcode-->
<p>Four behaves exactly as you would expect it to!</p>
<!--begin.rcode
result15 <- kmeans(d[,1:2], 5)
d$cluster15 <- as.factor(result15$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster15)) + ggtitle('kmeans result1 -- (k=5)')
end.rcode-->
<p>with 5 centres it tries to pick off a few weak points at the edge of one of the herds and call them a group.</p>

<h3>Examining the results</h3>
<div class="from-help">
<h5>Value</h5>

<p>kmeans returns an object of class "kmeans" which has a print and a fitted method. It is a list with components:</p>
<table>
<tr><td>cluster</td><td>A vector of integers (from 1:k) indicating the cluster to which each point is allocated.</td></tr>
<tr><td>centers</td><td>A matrix of cluster centres.</td></tr>
<tr><td>totss</td><td>The total sum of squares.</td></tr>
<tr><td>withinss</td><td>Vector of within-cluster sum of squares, one component per cluster.</td></tr>
<tr><td>tot.withinss</td><td>Total within-cluster sum of squares, i.e., sum(withinss).</td></tr>
<tr><td>betweenss</td><td>The between-cluster sum of squares, i.e. totss-tot.withinss.</td></tr>
<tr><td>size</td><td>The number of points in each cluster.</td></tr>
<table>
</div>
<!--begin.rcode
res.comp <- as.data.frame ( rbind(  
                    c(2 ,result12$tot.withinss)
                  , c(3 ,result13$tot.withinss)
                  , c(4 ,result14$tot.withinss)
                  , c(5 ,result15$tot.withinss)
                    ) )

ggplot(res.comp,aes(res.comp[,1] , as.numeric(res.comp[,2]) ))+
       geom_line()
end.rcode-->
<p>The Total within-cluster sum of squares decreases rapidly as it finds the 'real' clusters, and then levels off as it starts to make a stab at what might be a cluster.</p>
<h3>Results</h3>
<p>Here are the results...note the algorithm found clusters whose means are close to the true means of our synthetic clusters</p>
<!--begin.rcode
result1
end.rcode-->
<p>plot results...we are looking good</p>
<!--begin.rcode
d$cluster1 <- as.factor(result1$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster1)) + ggtitle('kmeans result1 -- success!\n(k=4)')
end.rcode-->
<p>suppose we repeat these steps...what do you expect to happen?</p>
<!--begin.rcode
result2 <- kmeans(d[,1:2], 4)
end.rcode-->
<p>Notice that the fit got worse!<br>
(e.g. large decrease in between_SS / total_SS...also cluster means are not as good as before)</p>
<!--begin.rcode
result2
end.rcode-->
<p>and this scatterplot shows that something is obviously not right...what happened?</p>
<!--begin.rcode
d$cluster2 <- as.factor(result2$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster2)) + ggtitle('kmeans result2 -- trouble\n(k=4)')
end.rcode-->
<p>This instability is a result of the random initial seeds that the clustering algorithm uses if two initial seeds begin in the same cluster, then the algorithm will have difficulty finding all the clusters (in particular, the cluster which doesn't contain an initial seed will be difficult to identify)<br>
(note that in any case, the algorithm will still return exactly as many clusters as you asked it to!)</p>

</p>So how can we create a more stable clustering fit? by repeating the fit several times and taking an average (this is effectively an ensemble clustering technique...we will talk about ensemble methods in more detail later)</p>
<!--begin.rcode
result3 <- kmeans(d[,1:2], 4, nstart=10)
result3
d$cluster3 <- as.factor(result3$cluster)
ggplot(d, aes(x=x, y=y)) + geom_point(aes(colour=cluster3)) + ggtitle('kmeans result3 -- stable convergence\n(k=4, nstart=10)')
end.rcode-->
<p>What happens if we introduce a new length scale into the problem? how many clusters are in the dataset now?</p>
<!--begin.rcode
d2 <- rbind(d[,1:3], data.frame(x=1000+rnorm(20,0,50), y=1000+rnorm(20,0,50), label=as.factor(rep(5, each=20))))
ggplot(d2, aes(x=x, y=y)) + geom_point(aes(colour=label)) + ggtitle('d2 -- multiple length scales')
end.rcode-->
<p>as you can see, things go haywire...recall that clustering results are kind of a heuristic<br>
(in particular, not invariant to a change in units!)</p>
<!--begin.rcode
result4 <- kmeans(d2[,1:2], 5, nstart=10)
d2$cluster4 <- as.factor(result4$cluster)
ggplot(d2, aes(x=x, y=y)) + geom_point(aes(colour=cluster4)) + ggtitle('kmeans result4 -- trouble\n(k=5, nstart=10)')
end.rcode-->
<p>Lets standardis the variables and see what happens</p>
<!--begin.rcode
ds <- data.frame(scale(d2[,1:2],center =TRUE , scale=TRUE))
result5 <- kmeans(ds, 4, nstart=10)
ds$cluster4 <- as.factor(result5$cluster)
ggplot(ds, aes(x=x, y=y)) + geom_point(aes(colour=cluster4)) + ggtitle('kmeans result5 -- trouble\n(k=5, nstart=10)')
end.rcode-->
<p>now let's try k-means clustering with the iris dataset</p>
<!--begin.rcode
iris.result <- kmeans(iris[,1:4], 3)
end.rcode-->
<p>look at clustering results...you can already tell something is up</p>
<!--begin.rcode
iris.result$cluster
end.rcode-->
<p>Combine clustering results with input data (as factor)</p>
<p>Let's look at the scatterplots of clustering results & true labels together (using package gridExtra)</p>
<p>First install this guy</p>
<!--begin.rcode
if("gridExtra" %in% rownames(installed.packages()) == FALSE) {install.packages("gridExtra")}
library(gridExtra)
end.rcode-->
<p>now create our two scatterplots...note that ggplot returns an *object* which can be stored!</p>
<!--begin.rcode
iris2 <- cbind(iris, cluster=as.factor(iris.result$cluster))
p1 <- ggplot(iris2, aes(x=Sepal.Width, y=Petal.Width)) + geom_point(aes(colour=cluster)) + ggtitle('clustering results')
p2 <- ggplot(iris, aes(x=Sepal.Width, y=Petal.Width)) + geom_point(aes(colour=Species)) + ggtitle('true labels')

grid.arrange(p1, p2)
end.rcode-->
<p>so what is going on here?</p>
<p>although clustering is a unsupervised learning technique it does an OK job at matching known groups</p>
</body>
</html>
